{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import html2text\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "\n",
    "## Set OpenAI ApiKey\n",
    "\n",
    "API_KEY = ''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nivsi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nivsi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea:\n",
    " - Is this all in plaintext or is there some structure here?\n",
    " - Could we have multiple connectors? So the worst is plaintext, but if you can augment it with K9DB or RuleKeeper or something, it gets better?\n",
    "\n",
    "\n",
    "Question Gen\n",
    "- https://www.asd5.org/cms/lib4/WA01001311/Centricity/Domain/145/Costas.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=API_KEY)\n",
    "def ask_gpt(systemprompt, userprompt):\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": systemprompt},\n",
    "        {\"role\": \"user\", \"content\": userprompt}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message\n",
    "\n",
    "# Use this for now -- Gradescope policy, for instance, is FAR TOO LONG for GPT-3 completions. Need to think\n",
    "# of a way to split it up.\n",
    "def get_hardcoded_policy(url):\n",
    "    return '''Gradescope collects the following types of data and shares them with the corresponding entities:\n",
    "\n",
    "- Personal information: This may include names, email addresses, and other identifying details provided by users. It is shared with Gradescope's internal team, employees, and authorized service providers who need access to the information to perform their duties and improve the service. For example, the internal team may use personal information to verify user identities, provide customer support, and communicate important updates. Authorized service providers may include hosting providers, data storage providers, and analytics platforms who assist Gradescope in delivering and improving the service.\n",
    "- Usage data: This includes information about how users interact with the platform, such as the courses they enroll in, assignments they submit, and grades they receive. It is shared with Gradescope's internal team, employees, and authorized service providers who analyze the data to enhance the platform's features and functionality. This analysis helps Gradescope understand user preferences, identify areas for improvement, and make data-driven decisions to provide a better user experience. The internal team may also use usage data to monitor system performance, troubleshoot issues, and ensure the proper functioning of the platform.\n",
    "\n",
    "Gradescope takes appropriate measures to safeguard the collected data and respects user privacy in accordance with its privacy policy. These measures include implementing technical and organizational security measures to protect against unauthorized access, maintaining strict data confidentiality obligations for employees and service providers, and regularly monitoring and updating security practices to align with industry standards. It is important for users to review and understand the privacy policy to be aware of their rights and choices regarding their data.\n",
    "Gradescope does not sell or share personal information with third parties for marketing purposes. The collected data is used solely for the purposes stated in the privacy policy and to provide and improve the Gradescope service. Users can trust that their data is handled with care and in compliance with applicable laws and regulations.'''\n",
    "\n",
    "def get_policy_from_web(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        text_with_html = soup.get_text()\n",
    "        text_without_html = html2text.html2text(text_with_html)\n",
    "        return text_without_html\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "\n",
    "\n",
    "# VERY BASIC, from https://www.geeksforgeeks.org/python-text-summarizer/\n",
    "def summarizer(text):\n",
    "    \n",
    "    # Tokenizing the text \n",
    "    stopWords = set(stopwords.words(\"english\")) \n",
    "    words = word_tokenize(text) \n",
    "    \n",
    "    # Creating a frequency table to keep the score of each word \n",
    "    \n",
    "    freqTable = dict() \n",
    "    for word in words: \n",
    "        word = word.lower() \n",
    "        if word in stopWords: \n",
    "            continue\n",
    "        if word in freqTable: \n",
    "            freqTable[word] += 1\n",
    "        else: \n",
    "            freqTable[word] = 1\n",
    "    \n",
    "    # Creating a dictionary to keep the score of each sentence \n",
    "    sentences = sent_tokenize(text) \n",
    "    sentenceValue = dict() \n",
    "    \n",
    "    for sentence in sentences: \n",
    "        for word, freq in freqTable.items(): \n",
    "            if word in sentence.lower(): \n",
    "                if sentence in sentenceValue: \n",
    "                    sentenceValue[sentence] += freq \n",
    "                else: \n",
    "                    sentenceValue[sentence] = freq \n",
    "    \n",
    "    sumValues = 0\n",
    "    for sentence in sentenceValue: \n",
    "        sumValues += sentenceValue[sentence] \n",
    "    \n",
    "    # Average value of a sentence from the original text \n",
    "    \n",
    "    average = int(sumValues / len(sentenceValue)) \n",
    "    \n",
    "    # Storing sentences into our summary. \n",
    "    summary = '' \n",
    "    for sentence in sentences: \n",
    "        if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)): \n",
    "            summary += \" \" + sentence \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costas Levels of Questioning\n",
    "\n",
    "https://www.asd5.org/cms/lib4/WA01001311/Centricity/Domain/145/Costas.pdf\n",
    "\n",
    "- LEVEL\t1: Remember Define, Show Understanding\n",
    "- LEVEL 2: Use Understanding, Examine, Create\n",
    "- LEVEL 3: Decide, Show Understanding, Supportive Evidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_costas_level_prompt(level):\n",
    "    if level == 1:\n",
    "        return \"\"\n",
    "    elif level == 2:\n",
    "        return \"\"\n",
    "    elif level == 3:\n",
    "        return \"\"\n",
    "    return \"\"\n",
    "\n",
    "def gen_question(context, level):\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow 1\n",
    "- Ask Question (Gen? Mockup? C.R.O.W.D.? Costas?)\n",
    "- Student Answers\n",
    "- Extract student answer, compare with privacy policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will abstract out to various connectors\n",
    "def get_system_prompt(policy_url):\n",
    "\n",
    "    plaintext_policy = get_policy_from_web(policy_url)\n",
    "\n",
    "    def rough_num_words(s):\n",
    "        return s.count(\" \") + 1\n",
    "    \n",
    "    c = 0\n",
    "    ## HAck bc of OpenAI Limits\n",
    "    while rough_num_words(plaintext_policy) > 3000 or c > 2:\n",
    "        print(\"SUMMARIZE!\")\n",
    "        c += 1\n",
    "        plaintext_policy = summarizer(plaintext_policy)\n",
    "\n",
    "\n",
    "    PLAINTEXT_SYSTEM_PROMPT = ''' You are a tutor helping determine whether users correctly understand the privacy policy below:\n",
    "        [PRIVACY POLICY]\n",
    "        {p}\n",
    "\n",
    "        For this policy, the student was asked a [QUESTION] and responded with an [ANSWER]. \n",
    "        Determine whether [ANSWER] answers [QUESTION], whether it was correct or incorrect.\n",
    "        {{\n",
    "            'Relevant' : [True if [ANSWER] answers [QUESTION], False otherwise],\n",
    "            'Correct' : [True if the answer is correct, False if incorrect],\n",
    "            'Explanation' : [A 2 sentence explanation of why the answer was correct or incorrect],\n",
    "            'Confidence' : [Your confidence in this decision on a scale of 0 to 1, with 0 being not confident and 1 being completely certain.]\n",
    "        }}\n",
    "\n",
    "    '''.format(p = plaintext_policy)\n",
    "    return PLAINTEXT_SYSTEM_PROMPT\n",
    "\n",
    "\n",
    "def get_user_prompt(question, answer):\n",
    "    return ''' \n",
    "\n",
    "        The student was asked the following question:\n",
    "        \n",
    "        [QUESTION] {q}\n",
    "\n",
    "        and gave the following answer:\n",
    "        [ANSWER] {a}\n",
    "    '''.format(q = question, a = answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_qa(q, a, pp):\n",
    "    system_prompt = get_system_prompt(policy_url=pp)\n",
    "    user_prompt = get_user_prompt(question = q, answer = a)\n",
    "    return ask_gpt(systemprompt=system_prompt, userprompt= user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARIZE!\n",
      "SUMMARIZE!\n",
      "ChatCompletionMessage(content=\"{\\n    'Relevant': True,\\n    'Correct': False,\\n    'Explanation': 'The answer is incorrect. The privacy policy does not specifically mention sharing personal information with course instructors.',\\n    'Confidence': 0.9\\n}\", role='assistant', function_call=None)\n"
     ]
    }
   ],
   "source": [
    "# Lets test it out\n",
    "\n",
    "policy = 'https://www.gradescope.com/privacy'\n",
    "question = 'With whom does Gradescope share your personal information?'\n",
    "answer = \"Gradescope only shares my personal information with course instructors\"\n",
    "\n",
    "print(check_qa(q = question, a = answer, pp = policy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow 2:\n",
    "- Student can ask questions about the Privacy Explorer\n",
    "- Explorer uses GPT to answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will abstract out to various connectors\n",
    "def flow_2_system_prompt(policy_url):\n",
    "\n",
    "\n",
    "    plaintext_policy = get_policy_from_web(policy_url)\n",
    "\n",
    "    def rough_num_words(s):\n",
    "        return s.count(\" \") + 1\n",
    "    \n",
    "    c = 0\n",
    "    ## Hack bc of OpenAI Limits\n",
    "    while rough_num_words(plaintext_policy) > 3000 or c > 2:\n",
    "        print(\"SUMMARIZE!\")\n",
    "        c += 1\n",
    "        plaintext_policy = summarizer(plaintext_policy)\n",
    "\n",
    "\n",
    "    PLAINTEXT_SYSTEM_PROMPT = ''' You are a tutor helping answer user questions about the privacy policy below:\n",
    "        [PRIVACY POLICY]\n",
    "        {p}\n",
    "\n",
    "\n",
    "    '''.format(p = plaintext_policy)\n",
    "    return PLAINTEXT_SYSTEM_PROMPT\n",
    "\n",
    "\n",
    "def flow_2_user_prompt(question):\n",
    "    return ''' \n",
    "        For this [PRIVACY POLICY], the student asked the following [QUESTION] and responded with an [ANSWER]. \n",
    "        \n",
    "        [QUESTION] {q}\n",
    "\n",
    "        Answer this question using only information from [PRIVACY POLICY]:\n",
    "        {{\n",
    "            'Answer' : [A short answer to the question],\n",
    "            'Confidence' : [Your confidence in this decision on a scale of 0 to 1, with 0 being not confident and 1 being completely certain.]\n",
    "        }}\n",
    "    '''.format(q = question, a = answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARIZE!\n",
      "SUMMARIZE!\n",
      "ChatCompletionMessage(content=\"{\\n            'Answer': 'Gradescope shares personal information with third-party service providers, such as language translation service providers, who have contractual agreements to handle and secure the data with the same level of protection as Turnitin.',\\n            'Confidence': 0.9\\n        }\", role='assistant', function_call=None)\n"
     ]
    }
   ],
   "source": [
    "def gen_a(q, pp):\n",
    "    system_prompt = flow_2_system_prompt(policy_url=pp)\n",
    "    user_prompt = flow_2_user_prompt(question = q)\n",
    "    return ask_gpt(systemprompt=system_prompt, userprompt= user_prompt)\n",
    "\n",
    "print(gen_a(q = question,  pp = policy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
